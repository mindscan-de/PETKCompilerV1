/*
 * generated by Xtext 2.27.0
 */
package de.mindscan.ai.aidsl.generator

import de.mindscan.ai.aidsl.aiDsl.BASICTYPE
import de.mindscan.ai.aidsl.aiDsl.VMFieldElement
import de.mindscan.ai.aidsl.aiDsl.VMNodeDefinition
import de.mindscan.ai.aidsl.aiDsl.VMNodeFieldElements
import de.mindscan.ai.aidsl.aiDsl.VMNodeOpCodeElement
import de.mindscan.ai.aidsl.aiDsl.VMOverrideFieldElement
import de.mindscan.ai.aidsl.aiDsl.WorkflowDataDictionaryElement
import de.mindscan.ai.aidsl.aiDsl.WorkflowDefinition
import de.mindscan.ai.aidsl.aiDsl.WorkflowDefinitionApplyLLMTaskStatement
import de.mindscan.json.ExportToJson
import java.io.Serializable
import java.util.HashMap
import java.util.Map
import org.eclipse.emf.ecore.resource.Resource
import org.eclipse.xtext.generator.AbstractGenerator
import org.eclipse.xtext.generator.IFileSystemAccess2
import org.eclipse.xtext.generator.IGeneratorContext

/**
 * Generates code from your model files on save.
 * 
 * See https://www.eclipse.org/Xtext/documentation/303_runtime_concepts.html#code-generation
 */
class AiDslGenerator extends AbstractGenerator {

	override void doGenerate(Resource resource, IFileSystemAccess2 fsa, IGeneratorContext context) {
		
		// If we iterate over Model, we get each single file, but do we know the name of the file?
		// simply compile the workflows
		// for(model : resource.allContents.toIterable.filter(Model)) {
			// actually iwant to iterate over the model definiionts and extract the workflows.
			
			//  
			for(workflow : resource.allContents.toIterable.filter(WorkflowDefinition)) {
				fsa.generateFile(
					"workflows/" + workflow.name + ".json.workflow", workflow.compile(resource)
				)
			}
			
		// }
		
	}
	
	def CharSequence compile(WorkflowDefinition workflowDefinition, Resource resource) {
		// TODO: 
		
		val metadataMap = getCompiledMetadataMap(workflowDefinition)
		val executionMap = getCompiledExecutionInfoMap(workflowDefinition)
		// TODO: extend the input maps
		val nodedataMap = getCompiledNodedataMap(workflowDefinition)
		val edgedataMap = getCompiledEdgeDataMap(workflowDefinition)
		val datadictionaryMap = getCompiledDataDictionaryMap(workflowDefinition)
		

		// build workflow definition
		val fullyCompiledWorkflowMap = newLinkedHashMap(
			'__metadata' -> metadataMap,
			'execute' -> executionMap,
			'nodedata' -> nodedataMap,
			'edgedata' -> edgedataMap,
			'json_data_dictionary' -> datadictionaryMap			
		)
		
		// export workflow definition
		val export = new ExportToJson()
		return export.asJsonString( fullyCompiledWorkflowMap);
		
		// MAYBE LATER?
		// TODO: 
		// * implement the outer workflow definition
		// * then compile each statements
		// ---
		// fullyAnnotatedWorkflow = M2MTransformer.compileAndTransfrmWorkflowToFullAST(workflowDefinitin)
		//
		// build a DAG from the fully annotated workflow
		//
		// optimize the DAG
		//
		// then build the data structure and write it to json file.
	}
	
	protected def HashMap<String, Serializable> getCompiledDataDictionaryMap(WorkflowDefinition workflowDefinition) {
		val result = newLinkedHashMap()
		
		val datadictionary = workflowDefinition.datadictionary
//		for(datadictionaryelement : datadictionary.dataDictionaryElements) {
//			val map = comileDataDictionaryElement(datadictionaryelement)
//			
//			result.put(datadictionaryelement.name, map)				
//		}
		
		return result
	}
	
	protected def HashMap<String, String> comileDataDictionaryElement(WorkflowDataDictionaryElement datadictionaryelement) {
		var map=newHashMap()
		
		if(datadictionaryelement.extends !== null) {
			map = comileDataDictionaryElement(datadictionaryelement.extends)
		}
		
		// TODO: extract the string values, because the string includes the leading and tailing values
		for(keyvaluepair:datadictionaryelement.keyValuePairs) {
			
			// map.put(keyvaluepair.key, keyvaluepair.value.prepareStringForExport )
		}
		return map
	}
	
	protected def String prepareStringForExport(String stringValue) {
		if(stringValue === null) {
			return stringValue
		}
		
		// multiline string
		if(stringValue.startsWith("''") && stringValue.endsWith("''") && stringValue.length()>=4)  {
			// this is a multiline comment, we need to transform this and remove the leading whitespaces, at the beginning of the line
			
			return stringValue.substring("''".length(), stringValue.length()-"''".length());
		}
		
		// single line string
		if(stringValue.startsWith('"') && stringValue.endsWith('"') && stringValue.length()>=2)  {
			return stringValue.substring('"'.length(), stringValue.length()-'"'.length());
		}
		
		return stringValue
	}
	
	
	protected def HashMap<String, Serializable> getCompiledEdgeDataMap(WorkflowDefinition workflowDefinition) {
		val connections = newLinkedHashMap()

		val statementList = workflowDefinition.statements.toList
		for(var i=0;i<statementList.length - 1;i++) {
			val currentStatement = statementList.get(i)
			val follownodeMap = newLinkedHashMap()
			
			// depending on the type of the current node, we have to fill the followNodeMap
			val nextStatement = statementList.get(i+1)
			val nextElementList = newArrayList()
			nextElementList.add(nextStatement.llmtask.name)
			follownodeMap.put("next", nextElementList)
			
			connections.put(currentStatement.llmtask.name , follownodeMap)
		}		
		
		newHashMap(
			'__comment' -> "all the edges",
			'connections' -> connections
		)
	}
	
	protected def HashMap<String, Serializable> getCompiledNodedataMap(WorkflowDefinition workflowDefinition) {
		// for now we use the workflow, usually we should focus on those where the name of the resource matches the workflow name
		val workflowstatements = workflowDefinition.statements
		
		val compiledNodes = newArrayList()
		
		// compile nodedata_nodes
		// compile all statements
		for(workflowstatement : workflowstatements) {
			// basically we can compile each note individually
			// but maybe there are synthetic nodes in between
			compiledNodes.add(workflowDefinition.getCompiledStatement(workflowstatement ))
		}
		
		newHashMap(
			'__comment'->"all the nodes", 
			'nodes'-> compiledNodes
		)
	}
	
	def Map getCompiledStatement(WorkflowDefinition definition, WorkflowDefinitionApplyLLMTaskStatement statement) {
		
		val compiledStatementMap = newLinkedHashMap()
		
		val taskdefinition = statement.llmtask
		// this must be a unique name, in case the task is used multiple times, otherwise the followelements can't be properly calculated
		compiledStatementMap.put("taskname", taskdefinition.name)
		
		val annotation_interfaces = taskdefinition.annotation_interfaces
		
		// do them in order,
		// first the super, then the overrides 
		for (annotationinterface : annotation_interfaces) {
			val vmNodeDefinition = annotationinterface.name
			val precompiledInterfaceMap = getPrecompiledInterface(vmNodeDefinition)
			// integrate the precompiledInterfaceMap
			compiledStatementMap.putAll(precompiledInterfaceMap)
		}
		
		// combine the pre-compield annotation interfaces in correct order
		

		// This is the LlmTaskDefinition
		// this is the final override.		
		// build map and then merge map - currently we do this 
		// Override each of the dictionary elements, by the assignments of the task definitions
		for(variableAssigment : taskdefinition.assignments) {
			// TODO, preprocess the template string.
			// look at the prefix first \r\n\t, all other \r\n\t must be replaced as "\n", such that the  
			// TODO: extract the string values, because the string includes the leading and tailing values
			compiledStatementMap.put(variableAssigment.variablename, variableAssigment.template.prepareStringForExport)
		}
		
		// TODO: extra_stopwords
		compiledStatementMap.put("extra_stopwords", newLinkedList)
		
		// TODO: inputs
		compiledStatementMap.put("inputs", newLinkedList)
		
		// TODO: calculate output type (also handle type overrides)
		val outputnodelist = newLinkedList()
		for (noderesultassignment : statement.noderesultassigments.toList()) {
			val source = noderesultassignment.noderesultname
			val target = noderesultassignment.environmentresultname
			// TODO: this type must be correctly determined	
			val type = "string"
			
			val noderesultmapping = newLinkedHashMap()
			noderesultmapping.put('target', target)
			noderesultmapping.put('source', source)
			noderesultmapping.put('__datatype', type)
			
			outputnodelist.add(noderesultmapping)
		}
		
		compiledStatementMap.put("outputs", outputnodelist)

		
		return compiledStatementMap
	}
	
	def Map getPrecompiledInterface(VMNodeDefinition definition) {
		val result = newLinkedHashMap()
		// find the opcode if present
		for(fieldElement : definition.elements) {
			switch fieldElement {
				VMNodeOpCodeElement : result.put("type", fieldElement.opcode)
				VMNodeFieldElements : result.putAll(getPrecompiledInterfaceFieldEleements(fieldElement))
				//VMNodeInElements : ;
				//VMNodeOutElements : ;
				//default: break; 
			}
		}
		
		// find all 
		
		return result 
	}
	
	def Map getPrecompiledInterfaceFieldEleements(VMNodeFieldElements elements) {
		val result = newLinkedHashMap()
		
		for(fieldElement : elements.fieldELements) {
			switch fieldElement {
				// TODO: extract the string values, because the string includes the leading and tailing values
				VMFieldElement: result.put(fieldElement.name, fieldElement.defaultvalue.prepareStringForExport )
				// TODO: extract the string values, because the string includes the leading and tailing values
				VMOverrideFieldElement: result.put(fieldElement.name, fieldElement.defaultvalue.prepareStringForExport )
			}
		}
		
		return result
	}
	
	protected def HashMap<String, Serializable> getCompiledExecutionInfoMap(WorkflowDefinition workflowDefinition) {
		// shortcut...
		// calculate entry point
		// this must be done on the compiled DAG
		
		// compile entrypoint
		val statement = workflowDefinition.statements.toArray()
		val firststatement = statement.get(0) as WorkflowDefinitionApplyLLMTaskStatement
		val entrypointname = firststatement.llmtask.name
		
		// compile inputfields
		val workflowinputdefinition = workflowDefinition.input
		val inputfields = newHashMap()
		for(inputelement : workflowinputdefinition.uiElements) {
			// TODO build each inputelement
			val inputfieldname = inputelement.name
			val inputfielddatatype = inputelement.datatype as BASICTYPE
			val uilabel = inputelement.uielement.label
			val uitype = inputelement.uielement.uitype
			
			val inputinfo = newLinkedHashMap(
				'__uitype'->uitype, 
				'__datatype'->inputfielddatatype.typename,
				'label'->uilabel
			)
			
			inputfields.put(inputfieldname, inputinfo)
		}
		
		
		val executionMap = newHashMap(
			'entry'-> entrypointname,
			'inputfields' -> inputfields )
		return executionMap
	}
	
	protected def HashMap<String, String> getCompiledMetadataMap(WorkflowDefinition workflowDefinition) {
		// shortcut...
		// compile metadata
		
		return newHashMap( 
			'name' -> workflowDefinition.name, 
			'short_description' -> "", 
			'version' -> "1.0.0", 
			'__description'-> ""  )
	}
	
}
