/*
 * generated by Xtext 2.27.0
 */
package de.mindscan.ai.aidsl.generator

import de.mindscan.ai.aidsl.aiDsl.VMFieldElement
import de.mindscan.ai.aidsl.aiDsl.VMNodeDefinition
import de.mindscan.ai.aidsl.aiDsl.VMNodeFieldElements
import de.mindscan.ai.aidsl.aiDsl.VMNodeOpCodeElement
import de.mindscan.ai.aidsl.aiDsl.VMOverrideFieldElement
import de.mindscan.ai.aidsl.aiDsl.WorkflowDataDictionaryDefinition
import de.mindscan.ai.aidsl.aiDsl.WorkflowDataDictionaryElement
import de.mindscan.ai.aidsl.aiDsl.WorkflowDefinition
import de.mindscan.ai.aidsl.aiDsl.WorkflowDefinitionApplyLLMTaskStatement
import de.mindscan.json.ExportToJson
import java.io.Serializable
import java.util.HashMap
import java.util.Map
import org.eclipse.emf.ecore.resource.Resource
import org.eclipse.xtext.generator.AbstractGenerator
import org.eclipse.xtext.generator.IFileSystemAccess2
import org.eclipse.xtext.generator.IGeneratorContext

/**
 * Generates code from your model files on save.
 * 
 * See https://www.eclipse.org/Xtext/documentation/303_runtime_concepts.html#code-generation
 */
class AiDslGenerator extends AbstractGenerator {

	override void doGenerate(Resource resource, IFileSystemAccess2 fsa, IGeneratorContext context) {
		
		// Ifwe iterate over Model, we get each single file, but do we know the name of the file?
		// simply compile the workflows
		// for(model : resource.allContents.toIterable.filter(Model)) {
			// actually iwant to iterate over the model definiionts and extract the workflows.
			
			//  
			for(workflow : resource.allContents.toIterable.filter(WorkflowDefinition)) {
				fsa.generateFile(
					"workflows/" + workflow.name + ".json.workflow", workflow.compile(resource)
				)
			}
			
		// }
		
	}
	
	def CharSequence compile(WorkflowDefinition workflowDefinition, Resource resource) {
		// TODO: 
		
		val metadataMap = getCompiledMetadataMap(workflowDefinition)
		val executionMap = getCompiledExecutionInfoMap(workflowDefinition)
		// TODO: extend the input and output maps
		val nodedataMap = getCompiledNodedataMap(workflowDefinition)
		// TODO: implement this
		val edgedataMap = getCompiledEdgeDataMap(workflowDefinition)
		val datadictionaryMap = resource.getCompiledDataDictionaryMap(workflowDefinition)
		

		// build workflow definition
		val fullyCompiledWorkflowMap = newLinkedHashMap(
			'__metadata' -> metadataMap,
			'execute' -> executionMap,
			'nodedata' -> nodedataMap,
			'edgedata' -> edgedataMap,
			'json_data_dictionary' -> datadictionaryMap			
		)
		
		// export workflow definition
		val export = new ExportToJson()
		return export.asJsonString( fullyCompiledWorkflowMap);
		
		// MAYBE LATER?
		// TODO: 
		// * implement the outer workflow definition
		// * then compile each statements
		// ---
		// fullyAnnotatedWorkflow = M2MTransformer.compileAndTransfrmWorkflowToFullAST(workflowDefinitin)
		//
		// build a DAG from the fully annotated workflow
		//
		// optimize the DAG
		//
		// then build the data structure and write it to json file.
	}
	
	protected def HashMap<String, Serializable> getCompiledDataDictionaryMap(Resource resource, WorkflowDefinition workflowDefinition) {
		val result = newLinkedHashMap()
		
		val alldatadictionary = resource.allContents.toIterable.filter(WorkflowDataDictionaryDefinition)
		
		for(datadictionary:alldatadictionary) {
			for(datadictionaryelement : datadictionary.dataDictionaryElements) {
				val map = comileDataDictionaryElement(datadictionaryelement)
				
				result.put(datadictionaryelement.name, map)				
			}
		}
		
		return result
	}
	
	protected def HashMap<String, String> comileDataDictionaryElement(WorkflowDataDictionaryElement datadictionaryelement) {
		var map=newHashMap()
		
		if(datadictionaryelement.extends !== null) {
			map = comileDataDictionaryElement(datadictionaryelement.extends)
		}
		
		// TODO: extract the string values, because the string includes the leading and tailing values
		for(keyvaluepair:datadictionaryelement.keyValuePairs) {
			
			map.put(keyvaluepair.key, keyvaluepair.value.prepareStringForExport )
		}
		return map
	}
	
	protected def String prepareStringForExport(String stringValue) {
		if(stringValue === null) {
			return stringValue
		}
		
		// multiline string
		if(stringValue.startsWith("''") && stringValue.endsWith("''") && stringValue.length()>=4)  {
			// this is a multiline comment, we need to transform this and remove the leading whitespaces, at the beginning of the line
			
			return stringValue.substring("''".length(), stringValue.length()-"''".length());
		}
		
		// single line string
		if(stringValue.startsWith('"') && stringValue.endsWith('"') && stringValue.length()>=2)  {
			return stringValue.substring('"'.length(), stringValue.length()-'"'.length());
		}
		
		return stringValue
	}
	
	
	protected def HashMap<String, Serializable> getCompiledEdgeDataMap(WorkflowDefinition workflowDefinition) {
		// compile edgedata
		newHashMap(
			'__comment' -> "all the edges",
			'connections' -> newHashMap()
		)
	}
	
	protected def HashMap<String, Serializable> getCompiledNodedataMap(WorkflowDefinition workflowDefinition) {
		// for now we use the workflow, usually we should focus on those where the name of the resource matches the workflow name
		val workflowstatements = workflowDefinition.statements
		
		val compiledNodes = newArrayList()
		
		// compile nodedata_nodes
		// compile all statements
		for(workflowstatement : workflowstatements) {
			// basically we can compile each note individually
			// but maybe there are synthetic nodes in between
			compiledNodes.add(workflowDefinition.getCompiledStatement(workflowstatement ))
		}
		
		newHashMap(
			'__comment'->"all the nodes", 
			'nodes'-> compiledNodes
		)
	}
	
	def Map getCompiledStatement(WorkflowDefinition definition, WorkflowDefinitionApplyLLMTaskStatement statement) {
		
		val compiledStatementMap = newLinkedHashMap()
		
		val taskdefinition = statement.llmtask
		// this must be a unique name, in case the task is used multiple times, otherwise the followelements can't be properly calculated
		compiledStatementMap.put("taskname", taskdefinition.name)
		
		val annotation_interfaces = taskdefinition.annotation_interfaces
		
		// do them in order,
		// first the super, then the overrides 
		for (annotationinterface : annotation_interfaces) {
			val vmNodeDefinition = annotationinterface.name
			val precompiledInterfaceMap = getPrecompiledInterface(vmNodeDefinition)
			// integrate the precompiledInterfaceMap
			compiledStatementMap.putAll(precompiledInterfaceMap)
		}
		
		// combine the pre-compield annotation interfaces in correct order
		

		// This is the LlmTaskDefinition
		// this is the final override.		
		// build map and then merge map - currently we do this 
		// Override each of the dictionary elements, by the assignments of the task definitions
		for(variableAssigment : taskdefinition.assignments) {
			// TODO, preprocess the template string.
			// look at the prefix first \r\n\t, all other \r\n\t must be replaced as "\n", such that the  
			// TODO: extract the string values, because the string includes the leading and tailing values
			compiledStatementMap.put(variableAssigment.variablename, variableAssigment.template.prepareStringForExport)
		}
		
		// TODO: extra_stopwords
		compiledStatementMap.put("extra_stopwords", newLinkedList)
		
		// TODO: inputs
		compiledStatementMap.put("inputs", newLinkedList)
		
		// TODO: outputs
		compiledStatementMap.put("outputs", newLinkedList)

		
		return compiledStatementMap
	}
	
	def Map getPrecompiledInterface(VMNodeDefinition definition) {
		val result = newLinkedHashMap()
		// find the opcode if present
		for(fieldElement : definition.elements) {
			switch fieldElement {
				VMNodeOpCodeElement : result.put("type", fieldElement.opcode)
				VMNodeFieldElements : result.putAll(getPrecompiledInterfaceFieldEleements(fieldElement))
				//VMNodeInElements : ;
				//VMNodeOutElements : ;
				//default: break; 
			}
		}
		
		// find all 
		
		return result 
	}
	
	def Map getPrecompiledInterfaceFieldEleements(VMNodeFieldElements elements) {
		val result = newLinkedHashMap()
		
		for(fieldElement : elements.fieldELements) {
			switch fieldElement {
				// TODO: extract the string values, because the string includes the leading and tailing values
				VMFieldElement: result.put(fieldElement.name, fieldElement.defaultvalue.prepareStringForExport )
				// TODO: extract the string values, because the string includes the leading and tailing values
				VMOverrideFieldElement: result.put(fieldElement.name, fieldElement.defaultvalue.prepareStringForExport )
			}
		}
		
		return result
	}
	
	protected def HashMap<String, Serializable> getCompiledExecutionInfoMap(WorkflowDefinition workflowDefinition) {
		// shortcut...
		// calculate entry point
		// this must be done on the compiled DAG
		
		val statement = workflowDefinition.statements.toArray()
		val firststatement = statement.get(0) as WorkflowDefinitionApplyLLMTaskStatement
		val entrypointname = firststatement.llmtask.name
		
		// compile entrypoint
		// compile inputfields
		val executionMap = newHashMap(
			'entry'-> entrypointname,
			// TODO: calculate the input field definitions 
			// TODO: extend the DSL
			'inputfields' -> newHashMap() )
		return executionMap
	}
	
	protected def HashMap<String, String> getCompiledMetadataMap(WorkflowDefinition workflowDefinition) {
		// shortcut...
		// compile metadata
		
		return newHashMap( 
			'name' -> workflowDefinition.name, 
			'short_description' -> "", 
			'version' -> "1.0.0", 
			'__description'-> ""  )
	}
	
}
