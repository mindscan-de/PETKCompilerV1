/*
 * generated by Xtext 2.27.0
 */
package de.mindscan.ai.aidsl.generator

import de.mindscan.ai.aidsl.aiDsl.WorkflowDefinition
import de.mindscan.ai.aidsl.aiDsl.WorkflowDefinitionApplyLLMTaskStatement
import de.mindscan.json.ExportToJson
import java.io.Serializable
import java.util.HashMap
import org.eclipse.emf.ecore.resource.Resource
import org.eclipse.xtext.generator.AbstractGenerator
import org.eclipse.xtext.generator.IFileSystemAccess2
import org.eclipse.xtext.generator.IGeneratorContext
import java.util.Map

/**
 * Generates code from your model files on save.
 * 
 * See https://www.eclipse.org/Xtext/documentation/303_runtime_concepts.html#code-generation
 */
class AiDslGenerator extends AbstractGenerator {

	override void doGenerate(Resource resource, IFileSystemAccess2 fsa, IGeneratorContext context) {
		// simply compile the workflows
		for(workflow : resource.allContents.toIterable.filter(WorkflowDefinition)) {
			fsa.generateFile(
				"workflows/" + workflow.name + ".json.workflow", workflow.compile(resource)
			)
		}
	}
	
	def CharSequence compile(WorkflowDefinition workflowDefinition, Resource resource) {
		// TODO: 
		
		val metadataMap = getCompiledMetadataMap(workflowDefinition)
		val executionMap = getCompiledExecutionInfoMap(workflowDefinition)
		val nodedataMap = getCompiledNodedataMap(workflowDefinition)
		val edgedataMap = getCompiledEdgeDataMap(workflowDefinition)
		val datadictionaryMap = getCompiledDataDictionaryMap(workflowDefinition)

		// build workflow definition
		val fullyCompiledWorkflowMap = newLinkedHashMap(
			'__metadata' -> metadataMap,
			'execute' -> executionMap,
			'nodedata' -> nodedataMap,
			'edgedata' -> edgedataMap,
			'json_data_dictionary' -> datadictionaryMap			
		)
		
		// export workflow definition
		val export = new ExportToJson()
		return export.asJsonString( fullyCompiledWorkflowMap);
		
		// MAYBE LATER?
		// TODO: 
		// * implement the outer workflow definition
		// * then compile each statements
		// ---
		// fullyAnnotatedWorkflow = M2MTransformer.compileAndTransfrmWorkflowToFullAST(workflowDefinitin)
		//
		// build a DAG from the fully annotated workflow
		//
		// optimize the DAG
		//
		// then build the data structure and write it to json file.
	}
	
	protected def HashMap<String, String> getCompiledDataDictionaryMap(WorkflowDefinition workflowDefinition) {
		
		// compile json_data_dictionary
		return newHashMap()
	}
	
	protected def HashMap<String, Serializable> getCompiledEdgeDataMap(WorkflowDefinition workflowDefinition) {
		// compile edgedata
		newHashMap(
			'__comment' -> "all the edges",
			'connections' -> newHashMap()
		)
	}
	
	protected def HashMap<String, Serializable> getCompiledNodedataMap(WorkflowDefinition workflowDefinition) {
		// for now we use the workflow, usually we should focus on those where the name of the resource matches the workflow name
		val workflowstatements = workflowDefinition.statements
		
		val compiledNodes = newArrayList()
		
		// compile nodedata_nodes
		// compile all statements
		for(workflowstatement : workflowstatements) {
			// basically we can compile each note individually
			// but maybe there are synthetic nodes in between
			compiledNodes.add(workflowDefinition.getCompiledStatement(workflowstatement ))
		}
		
		newHashMap(
			'__comment'->"all the nodes", 
			'nodes'-> compiledNodes
		)
	}
	
	def Map getCompiledStatement(WorkflowDefinition definition, WorkflowDefinitionApplyLLMTaskStatement statement) {
		
		val compiledStatementMap = newLinkedHashMap()
		
		// TODO find the extended super node dictionaries
		val taskdefinition = statement.llmtask
		
		val annotation_interfaces = taskdefinition.annotation_interfaces
		
		// do them in order,
		// first the super, then the overrides 
		for (annotationinterface : annotation_interfaces) {
			
		}

		// this is the final override.		
		// build map and then merge map - currently we do this 
		// Override each of the dictionary elements, by the assignments of the task definitions
		for(variableAssigment : taskdefinition.assignment) {
			// TODO, preprocess the template string.
			compiledStatementMap.put(variableAssigment.variablename, variableAssigment.template)
		}
		
		return compiledStatementMap
	}
	
	protected def HashMap<String, Serializable> getCompiledExecutionInfoMap(WorkflowDefinition workflowDefinition) {
		// shortcut...
		// calculate entry point
		// this must be done on the compiled DAG
		
		val statement = workflowDefinition.statements.toArray()
		val firststatement = statement.get(0) as WorkflowDefinitionApplyLLMTaskStatement
		val entrypointname = firststatement.llmtask.name
		
		// compile entrypoint
		// compile inputfields
		val executionMap = newHashMap(
			'entry'-> entrypointname,
			// TODO: calculate the input field definitions 
			// TODO: extend the DSL
			'inputfields' -> newHashMap() )
		return executionMap
	}
	
	protected def HashMap<String, String> getCompiledMetadataMap(WorkflowDefinition workflowDefinition) {
		// shortcut...
		// compile metadata
		
		return newHashMap( 
			'name' -> workflowDefinition.name, 
			'short_description' -> "", 
			'version' -> "1.0.0", 
			'__description'-> ""  )
	}
	
}
